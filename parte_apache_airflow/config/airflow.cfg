[core]
# The folder where airflow should store its log files
dags_folder = /opt/airflow/dags
base_log_folder = /opt/airflow/logs
plugins_folder = /opt/airflow/plugins

# Executor: SequentialExecutor for SQLite compatibility
executor = SequentialExecutor

# Database connection will be set via environment variable AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
# Format: postgresql+psycopg2://user:password@host:port/dbname

# Don't load example DAGs
load_examples = False

# Parallelism
parallelism = 8
max_active_runs_per_dag = 2
max_active_tasks_per_dag = 8

[webserver]
# Bind to all interfaces (required for ECS)
web_server_host = 0.0.0.0
web_server_port = 8080

# Base URL will be set by ALB
base_url = http://localhost:8080

# Secret key will be set via environment variable AIRFLOW__WEBSERVER__SECRET_KEY
# Generate with: python3 -c 'import secrets; print(secrets.token_hex(32))'

# Enable health check endpoint
enable_proxy_fix = True

[scheduler]
# Scheduler heartbeat
scheduler_heartbeat_sec = 5

# Number of times to try running a task instance
max_tis_per_query = 512

# How often (in seconds) to scan the DAGs directory for new files
dag_dir_list_interval = 300

[api]
# Enable authentication
auth_backend = airflow.api.auth.backend.basic_auth

[logging]
# Use remote logging to S3 (optional, can be enabled later)
# remote_logging = True
# remote_base_log_folder = s3://your-bucket/logs
# remote_log_conn_id = aws_default

# Console logging
logging_level = INFO
fab_logging_level = WARNING
colored_console_log = False

[metrics]
# CloudWatch metrics (can be enabled if needed)
statsd_on = False

[secrets]
# Use AWS Systems Manager Parameter Store for secrets
backend = airflow.providers.amazon.aws.secrets.systems_manager.SystemsManagerParameterStoreBackend
backend_kwargs = {"connections_prefix": "/airflow/connections", "variables_prefix": "/airflow/variables"}
